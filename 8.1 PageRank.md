## PageRank Overview

- write a simple web page crawler
- compute a simple version of Google's Page Rank algorithm
- visualise the resulting network

#### Search Engine Architecture
- web crawling: browse the web in a methodical, automated manner; create a copy of all the visited pages for later processing by a search engine that will index the downloaded pages to provide fast searches
	- retrieve a page
	- look through the page for links
	- add the links to a list of "to be retrieved" sites
	- repeat...
- web crawling policy:
	- selection policy: states which pages to download
	- re-visit policy: states when to check for changes to the pages
	- politeness policy: states how to avoid overloading web sites
	- parallelisation policy: states how to coordinate distributed web crawlers
- robots.txt
	- a way for a web server to communicate with web crawlers
	- an informal and voluntary standard
	- sometimes people make a "spider trap" to catch "bad" spiders
	```
	User-agent: *
	Disallow: /cgi-bin/
	Disallow: /images/
	Disallow: /tmp/
	Disallow: /private/
	```


- index building: search engine indexing collects, parses, and stores data to facillitate fast and accurate information retrieval; without an index, the search engine would scan every document in the corpus, which would require considerable time and computing power

![[pagerank.png]]


- searching 